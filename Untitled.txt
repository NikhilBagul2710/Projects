






























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































# PR 01
import pandas as pd
import matplotlib.pylab as plt
import numpy as np

df = pd.read_csv("autodata.csv")

df.head(5)

df.tail(5)

df.info()

df.describe()

df.isnull()

df.isnull().sum()

df.notnull()

df.notnull().sum()

# calculate the mean vaule for "stroke" column
avg_stroke = df["stroke"].astype("float").mean(axis = 0)
print("Average of stroke:", avg_stroke)
# replace NaN by mean value in "stroke" column
df["stroke"].replace(np.nan, avg_stroke, inplace = True)

avg_hp = df["horsepower"].astype("float").mean(axis = 0)
print("Average of horsepower:", avg_hp)

df["horsepower"].replace(np.nan, avg_hp, inplace = True)

avg_rpm = df["peak-rpm"].astype("float").mean(axis = 0)
print("Average of peak_rpm:", avg_rpm)

df["peak-rpm"].replace(np.nan, avg_hp, inplace = True)

df['num-of-doors'].value_counts()

df['num-of-doors'].value_counts().idxmax()

#replace the missing 'num-of-doors' values by the most frequent 
df["num-of-doors"].replace(np.nan, "four", inplace=True)
# simply drop whole row with NaN in "horsepower-binned" column
df.dropna(subset=["horsepower-binned"], axis=0, inplace=True)
# reset index, because we droped two rows
df.reset_index(drop=True, inplace=True)

df.isnull().sum()

df['city-L/100km'] = 235/df["city-mpg"]
df.head()

df['highway-L/100km'] = 235/df["highway-mpg"]
df.head()

df['length'] = df['length']/df['length'].max()
df['width'] = df['width']/df['width'].max()

df['height'] = df['height']/df['height'].max() 
df[["length","width","height"]].head()

df.columns

df['aspiration'].value_counts()

dummy_variable_1 = pd.get_dummies(df["aspiration"])
dummy_variable_1.head()

df = pd.concat([df, dummy_variable_1], axis=1)
df.drop("aspiration", axis = 1, inplace=True)

df.head()

df["horsepower"]=df["horsepower"].astype(float, copy=True)

%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(df["horsepower"])
plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")

bins = np.linspace(min(df["horsepower"]), max(df["horsepower"]), 4)
bins

group_names = ['Low', 'Medium', 'High']

df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )
df[['horsepower','horsepower-binned']].head(20)

df["horsepower-binned"].value_counts()

%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, df["horsepower-binned"].value_counts())

# set x/y labels and plot title
plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")

df["peak-rpm"]=df["peak-rpm"].astype(float, copy=True)

%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(df["peak-rpm"])
plt.pyplot.xlabel("peak-rpm")
plt.pyplot.ylabel("count")
plt.pyplot.title("Peak-rpm bins")

bins = np.linspace(min(df["peak-rpm"]), max(df["peak-rpm"]), 4)
bins

group_names1 = ['Low', 'Medium', 'High']

df['peakrpm-binned'] = pd.cut(df['peak-rpm'], bins, labels=group_names, include_lowest=True )
df[['peak-rpm','peakrpm-binned']].head(20)

df["peakrpm-binned"].value_counts()

%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, df["peakrpm-binned"].value_counts())
# set x/y labels and plot title
plt.pyplot.xlabel("Peak-rpm")
plt.pyplot.ylabel("count")
plt.pyplot.title("peak-rpm bins")

df["wheel-base"]=df["wheel-base"].astype(float, copy=True)

%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(df["wheel-base"])
plt.pyplot.xlabel("wheel-base")
plt.pyplot.ylabel("count")
plt.pyplot.title("Wheel-base bins")

bins = np.linspace(min(df["wheel-base"]), max(df["wheel-base"]), 4)
bins

group_names = ['Low', 'Medium', 'High']

df['wheelbase-binned'] = pd.cut(df['wheel-base'], bins, labels=group_names, include_lowest=True )
df[['wheel-base','wheelbase-binned']].head(20)

df["wheelbase-binned"].value_counts()

%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, df["wheelbase-binned"].value_counts())
# set x/y labels and plot title
plt.pyplot.xlabel("Wheelbase")
plt.pyplot.ylabel("count")
plt.pyplot.title("Wheelbase bins")



-----------------------------------------------------------------------------------------------------------

#PR 02
import pandas as pd #importing pandas
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
df = pd.read_csv("/content/student.csv") #reading the dataset file
df

#Checking the total count of rows of columns having null values
df.isnull().sum()

#displaying the rows with null values
df[df.isnull().any(axis=1)]

#Filling the null values with the mean of that column
df['Sem1'] = df['Sem1'].fillna(df['Sem1'].mean())
df

#Filling the null values with the mean of that column
df['Sem1'] = df['Sem1'].fillna(df['Sem1'].mean())
df

#Filling the null values with "Missing" word
df['Sem3'] = df['Sem3'].fillna("Missing")
df

#Droping the rows with null values
df.dropna()

df["Sem2"].min()

print(np.where(df["Sem2"]<20))

x=df['Sem2']
y=df['average Score']
plt.scatter(x,y)
plt.show()


x=df["Sem2"]
y=df["average Score"]
z=df["Roll No"]
fig=plt.figure()
ax=fig.add_axes([0,0,1,1])
ax.scatter(z,x,color='r')
ax.scatter(z,y,color='g')
ax.set_xlabel('Sem2')
ax.set_ylabel('average Score')
ax.set_title('Scatter plot')
plt.show()

z=np.abs(stats.zscore(df['Sem1']))
z2=np.abs(stats.zscore(df['Sem2']))
z3=np.abs(stats.zscore(df['Sem3']))
z4=np.abs(stats.zscore(df['Sem4']))
print(z)

Q1=np.quantile(df['Sem1'],0.25)
Q3=np.quantile(df['Sem1'],0.75)
IQR=Q3-Q1
print("Q1 :: ",Q1)
print("Q3 :: ",Q3)
print("IQR :: ",IQR)

Q1=np.quantile(df['Sem2'],0.25)
Q3=np.quantile(df['Sem2'],0.75)
IQR=Q3-Q1
print("Q1 :: ",Q1)
print("Q3 :: ",Q3)
print("IQR :: ",IQR)
upper_2 = Q3 + 1.5*IQR
lower_2 = Q1 - 1.5*IQR
print("lower bound :: ",lower_2)
print("upper bound :: ",upper_2)

print(np.where(df['Sem2']<lower_2))

print(np.where(df['Sem2']>upper_2))

df.drop([10], inplace=True)
print(df)

Q4 = np.quantile(df['Sem4'],0.90)
Q5 = np.quantile(df['Sem4'],0.10)
IQR=Q4-Q5
print("Q4 :: ",Q4)
print("Q5 :: ",Q5)
print("IQR :: ",IQR)
upper_bound = Q4 + 1.5*IQR
lower_bound = Q5 - 1.5*IQR
print("lower bound :: ",lower_bound)
print("upper bound :: ",upper_bound)

print(df)

sns.boxplot(y=data['First year:   Sem 1'])

sns.boxplot(y=data['First year:   Sem 2'])

sns.boxplot(y=data["Second year:   Sem 1"])

sns.boxplot(y=data["Second year:   Sem 2"])

----------------------------------------------------------------------------------------------------------------------

#PR 03
# Import the required libraries
import pandas as pd
import numpy as np
import sklearn
from sklearn import datasets

iris = datasets.load_iris()
iris

df = pd.DataFrame(iris['data'])
df.head()

df[4] = iris['target']
df.head()

# Adding column names
df.rename(columns = {0:'SepalLengthCm', 1:'SepalWidthCm', 2:'PetalLengthCm', 3:'PetalWidthCm', 4:'Species'}, inplace = True)
df.head()

df.describe()

df.shape

df.mean()

df.median()

# Calculated only for categorical data
df.Species.mode()

df.groupby(['Species']).count()

df.SepalLengthCm.std()

df.SepalWidthCm.std()

df.PetalLengthCm.std()

df.PetalWidthCm.std()

---------------------------
#checking that categorical data in the dataset
df.select_dtypes(include='object')

#converting categorical data into numerical data
encoder = preprocessing.LabelEncoder()
df["Species"]=encoder.fit_transform(df["Species"])
df

#using groupby function to group data
data = df.groupby(["Species"])
data.groups.keys()  #checking how many groups are formed

df.groupby(["Species"])["SepalWidthCm"].std()

df.groupby(["Species"])["SepalWidthCm"].mean()

df.groupby(["Species"])["SepalWidthCm"].min()

df.groupby(["Species"])["SepalWidthCm"].max()

df.groupby(["Species"])["SepalWidthCm"].var()

df.groupby(["Species"])["SepalWidthCm"].std()


df.groupby(["Species"])["SepalWidthCm"].apply(statistics.mode)

data = df.groupby(["Species"])
data.agg([np.sum, np.mean, np.max])

df.groupby(["Species"])["SepalWidthCm"].agg(pd.Series.mode)


-----------------------------------------------------------------------------------------------------------------------

# PR 04

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
bostonData = load_boston()

df = pd.DataFrame(bostonData.data, columns=bostonData.feature_names)
df['MEDV'] = bostonData.target
df

sns.set_palette('ocean')
sns.distplot(df['MEDV'], bins=30)

# Correlation Matrix
plt.figure(figsize=(20,10))     #   remember this line, it is useful to change the resolution of the plotted graph
correlationMatrix = df.corr().round(2)
sns.heatmap(data=correlationMatrix, annot=True)

features = ['LSTAT', 'RM']
target = df['MEDV']
for row, col in enumerate(features):

    plt.figure(figsize=(20,10))
    sns.regplot(data=df, x=df[col], y=target)
    plt.ylabel('MEDV')  
    
#preparing the data for training model
X = pd.DataFrame(np.c_[df['LSTAT'], df['RM']], columns= ['LSTAT', 'RM'])
Y = target
from sklearn.model_selection import train_test_split as t
# define 4 variables for training and testing x and y :
xtrain, xtest, ytrain, ytest = t(X, Y, test_size = 0.2, random_state = 5)
print(xtrain.shape)
print(xtest.shape)
print(ytrain.shape)
print(ytest.shape)

# model fitting
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error as mse
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
model = LR.fit(xtrain, ytrain)
# model evalutaion for training set
ytrain_prediction = LR.predict(xtrain)
rmse_train = (np.sqrt(mse(ytrain, ytrain_prediction)))
r2_train = r2_score(ytrain, ytrain_prediction)
#model evaluation for testing set
ytest_prediction = LR.predict(xtest)
rmse_test = (np.sqrt(mse(ytest, ytest_prediction)))
r2_test = r2_score(ytest, ytest_prediction)

print("The Model performance for training set")
print("--------------------------------------")
print('RMSE of training set: ', rmse_train)
print('R2 score of training set: ', r2_train, "\n\n")
print("The Model performance for testing set")
print("--------------------------------------")
print('RMSE of testing set: ', rmse_test)
print('R2 score of testing set: ', r2_test)

# Column Headings
CRIM - per capita crime rate by town
ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
INDUS - proportion of non-retail business acres per town.
CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
NOX - nitric oxides concentration (parts per 10 million)
RM - average number of rooms per dwelling
AGE - proportion of owner-occupied units built prior to 1940
DIS - weighted distances to five Boston employment centres
RAD - index of accessibility to radial highways
TAX - full-value property-tax rate per $10,000
PTRATIO - pupil-teacher ratio by town
B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
LSTAT - % lower status of the population
MEDV - Median value of owner-occupied homes in $1000's

----------------------------------
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
#Boston Dataset

boston = load_boston()

data = pd.DataFrame(boston.data)
data.head()

data['Price'] = boston.target
print(data)

x = boston.data
y = boston.target

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size =0.2,random_state = 0)

classifier = LinearRegression()
classifier.fit(xtrain, ytrain)

y_pred = classifier.predict(xtest)

plt.scatter(ytest, y_pred, c = 'green')
plt.xlabel("Price: in $1000's")
plt.ylabel("Predicted value")
plt.title("True value vs predicted value : Linear Regression")
plt.show()

mse = mean_squared_error(ytest, y_pred)
print("Mean Square Error : ", mse)


--------------------------------------------------------------------------------------------------------

#PR 05
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv("Social_Network_Ads.csv")
df.head()

df.dtypes

(df.isnull()).sum()

df.info()

sns.countplot(df['Purchased'])
plt.title('Distribution of Purchased or not')
plt.xlabel('Purchased or not')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize = (10,6))
plt.hist(df['Age'], bins  = 6, color = 'blue', rwidth = 0.98)
plt.title('Distribution of Age')
plt.xlabel('Different Ages')
plt.ylabel('Frequency')

X = df.iloc[:,[2,3]].values
print(X)
#Age and Salary

# dependent variable
y = df.iloc[:,4].values
print(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 0)
#test_size =0.25 means 25% data of whole dataset will be used for training and rest of for testing

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test  = sc.transform(X_test)

from sklearn.linear_model import LogisticRegression
classifier=LogisticRegression()
classifier.fit(X_train,y_train)

y_pred = classifier.predict(X_test)
print(y)
print(y_pred)

from sklearn.metrics import confusion_matrix,accuracy_score
cm1 = confusion_matrix(y_test,y_pred)
print('Confusion Matrix: ')
print(cm1)   
ac1 = accuracy_score(y_test, y_pred)*100
print('Accuracy Score:')
print(ac1)

tp=cm1[0][0]
tn=cm1[1][1]
fp=cm1[1][0]
fn=cm1[0][1]
total=tp+tn+fp+fn

error_rate=(fp+fn)/(total)
print('error rate: ')
print(error_rate)

 from sklearn.metrics import classification_report
print('                        classification report:')
print('')
print(classification_report(y_test,y_pred))


-------------------------------------------------
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score,confusion_matrix,accuracy_score,recall_score, classification_report

df = pd.read_csv("/content/Social_Network_Ads.csv")
df

#predicting whether the user will purchase the product or not
x = df.iloc[:, [2, 3]].values
y = df.iloc[:, 4].values

#spliting the dataset into x and y
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25, random_state = 0)

sc = StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.transform(xtest)
classifier = LogisticRegression(random_state = 0)
classifier.fit(xtrain, ytrain)

ypred = classifier.predict(xtest)

confusion=confusion_matrix(ytest,ypred)
confusion

accuracy=accuracy_score(ytest,ypred)
print(accuracy)

precision=precision_score(ytest,ypred,average='micro')
print(precision)

recall=recall_score(ytest,ypred,average='micro')
print(recall)

cl_report = classification_report(ytest,ypred)
print(cl_report)
-----------------------------------------------------------------------------------------------------

#PR 06

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("Iris.csv")
df.tail()
#df['Species'].unique()

df.isnull().sum()

df.dtypes

df.info()

df.describe()

# now preparing our model as per Gaussian Naive Bayesian
X = df.iloc[:,1:5] # X is the features in our dataset
y = df.iloc[:,-1]   # y is the Labels in our dataset
from sklearn.naive_bayes import GaussianNB

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) 

from sklearn.naive_bayes import GaussianNB
model = GaussianNB().fit(X_train, y_train)

y_pred= model.predict(X_test) #now predicting our model to our test dataset
print(y)
print(y_pred)

from sklearn.metrics import accuracy_score
# now calculating that how much accurate our model is with comparing our predicted values and y_test values
accuracy_score = accuracy_score(y_test, y_pred) 
print (accuracy_score)


from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test,y_pred)
print('Confusion Matrix: ')
print(cm)   
ac1 = accuracy_score(y_test, y_pred)*100
print('Accuracy Score:')
print(ac1)


#For Setosa Class
tp=cm[0][0]
fn=(cm[0][1])+(cm[0][2])
tn=(cm[1][1])+(cm[1][2])+(cm[2][1])+(cm[2][2])
fp=(cm[1][0])+(cm[2][0])
print('true positive: ',tp)
print('false positive: ',fp)
print('true negative: ',tn)
print('false negative: ',fn)
error_rate=(fp+fn)/(tp+tn+fp+fn)
print('error rate:', error_rate )

#For Versicolor Class
tp=cm[1][1]
fn=(cm[1][0])+(cm[1][2])
tn=(cm[0][0])+(cm[0][2])+(cm[2][0])+(cm[2][2])
fp=(cm[0][1])+(cm[2][1])
print('true positive: ',tp)
print('false positive: ',fp)
print('true negative: ',tn)
print('false negative: ',fn)
error_rate=(fp+fn)/(tp+tn+fp+fn)
print('error rate:', error_rate )

#For Virginca Class
tp=cm[1][2]
fn=(cm[2][0])+(cm[2][1])
tn=(cm[0][0])+(cm[0][1])+(cm[1][0])+(cm[1][1])
fp=(cm[0][2])+(cm[1][2])
print('true positive: ',tp)
print('false positive: ',fp)
print('true negative: ',tn)
print('false negative: ',fn)
error_rate=(fp+fn)/(tp+tn+fp+fn)
print('error rate:', error_rate )

 from sklearn.metrics import classification_report
print('                        classification report:')
print('')
print(classification_report(y_test,y_pred))

------------------------------------------------------------
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import precision_score, confusion_matrix, accuracy_score, recall_score, classification_report

df = pd.read_csv("Iris.csv")
df.head()

encoder = preprocessing.LabelEncoder()
df["Species"]=encoder.fit_transform(df["Species"])
df

x = df.iloc[:, [1, 4]].values
y = df.iloc[:, 5].values

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25, random_state = 0)

classifier = GaussianNB()
classifier.fit(xtrain, ytrain)

ypred=classifier.predict(xtest)

confusion=confusion_matrix(ytest,ypred)
confusion

accuracy=accuracy_score(ytest,ypred)
print(accuracy)

precision=precision_score(ytest,ypred,average='micro')
print(precision)

recall=recall_score(ytest,ypred,average='micro')
recall

------------------------------------------------------------------------------------------------------

#PR 07
# Tokenization
import numpy as np
import nltk
from nltk import sent_tokenize
from nltk import word_tokenize

text = "Hello this a sample text. Where are you from?"

tokens_sents = nltk.sent_tokenize(text)
print(tokens_sents)

tokens_words = nltk.word_tokenize(text)
print(tokens_words)

#stemming
from nltk.stem import PorterStemmer
from nltk.stem.snowball import SnowballStemmer

ps = PorterStemmer()
word = ("civilization")
ps.stem(word)

ps.stem("Workers")

stemmer = SnowballStemmer(language = "english")
stemmer.stem("Construction")

stemmer.stem("Randomly")

#lemminization
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()

print(lemmatizer.lemmatize("workers"))
print(lemmatizer.lemmatize("beaches"))

#POS tagging
text = "The striped bats are hanging on their feet for best"
tokens = nltk.word_tokenize(text)
print("Parts of Speech: ",nltk.pos_tag(tokens))

#Stopword removal
from nltk.corpus import stopwords

text = "Ich habe ein bisschen deutsch lernen"

tokens = word_tokenize(text.lower())
english_stopwords = stopwords.words('german')
tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords]
print("Text without stop words:", " ".join(tokens_wo_stopwords))

from sklearn.feature_extraction.text import TfidfVectorizer

d0 = 'Nothing ever goes as planned in this accursed world.'
d1 = 'The longer you live, the more you realize that the only things that truly exist in this reality are merely pain, suffering and futility.'
d2 = 'Listen, everywhere you look in this world, wherever there is light, there will always be shadows to be found as well.'
string = [d0, d1, d2]

tfidf = TfidfVectorizer()
result = tfidf.fit_transform(string)

print('\nWord indexes:')
print(tfidf.vocabulary_)
# display tf-idf values
print('\ntf-idf value:')
print(result)

---------------------------------------------------------
import nltk
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from  nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

sentence = "Hello I am Gayatri Deshmukh. I am from Nanded District. I will be an Engineer in few months."
#Tokenization
nltk.download('punkt')
tokenized_words = word_tokenize(sentence)
tokenized_sentences = sent_tokenize(sentence)
print(tokenized_words)
print(tokenized_sentences)

#Stop words removal
nltk.download('stopwords')
stop_words = stopwords.words('english')
cleaned_token = []
for i in tokenized_words:
  if i not in stop_words:
    cleaned_token.append(i)
print("Unclean version ", tokenized_words)
print("Clean Version", cleaned_token)

#Stemming
snowball_stemmer = SnowballStemmer('english')
stemmed_words = []
for i in tokenized_words:
    stemmed = snowball_stemmer.stem(i)
    stemmed_words.append(stemmed)
print(stemmed_words)

#Lemmatization
nltk.download('wordnet')
wordnet_lemmatizer = WordNetLemmatizer()
lemmatized_words = []
for i in tokenized_words:
    lemmatized = wordnet_lemmatizer.lemmatize(i)
    lemmatized_words.append(lemmatized)
print(lemmatized_words)

#Pos Tagging
# dt - determinnant
# NN - noun
# In - prep / conjunc
nltk.download('averaged_perceptron_tagger')
pos_tag = nltk.pos_tag(tokenized_words)
print(pos_tag)

d0 = "Good Morning"
d1 = "Do daily exercise in the morning "
d2 = "exercise is good for health"
series = [d0, d1, d2]
tfidf = TfidfVectorizer()
result = tfidf.fit_transform(series)
print("Word Indexing: ", tfidf.vocabulary_)
print("tf-idf in matrix form: \n", result.toarray())
---------------------------------------------------------------------------------

#PR 08

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/titanic_data.csv')
data

data.shape

data.describe()

data.describe(include = 'object')

data.isnull().sum()

data['Age'] = data['Age'].fillna(np.mean(data['Age']))

data['Cabin'] = data['Cabin'].fillna(data['Cabin'].mode()[0])

data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])

data.isnull().sum()

sns.countplot(data['Survived'])

sns.countplot(data['Pclass'])

sns.countplot(data['Embarked'])

sns.countplot(data['Sex'])

sns.boxplot(data['Age'])

sns.boxplot(data['Fare'])

sns.boxplot(data['Pclass'])

sns.boxplot(data['SibSp'])

sns.catplot(x= 'Pclass', y = 'Age', data=data, kind = 'box')

sns.catplot(x= 'Pclass', y = 'Fare', data=data, kind = 'strip')

sns.catplot(x= 'Sex', y = 'Fare', data=data, kind = 'strip')

sns.catplot(x= 'Sex', y = 'Age', data=data, kind = 'strip')

sns.pairplot(data)

sns.scatterplot(x = 'Fare', y = 'Pclass', hue = 'Survived', data = data)

sns.scatterplot(x = 'Survived', y = 'Fare', data = data)

sns.distplot(data['Age'])

sns.distplot(data['Fare'])

sns.jointplot(x = "Survived", y = "Fare", kind = "scatter", data = data)

tc = data.corr()
sns.heatmap(tc, cmap="YlGnBu")
plt.title('Correlation')

sns.catplot(x='Pclass', y='Fare', data=data, kind='bar')
----------------------------------------------------------------------------------------------------------

#PR 09

#   import libraries
import seaborn as sns
import matplotlib.pyplot as plt
titanic = sns.load_dataset("titanic")
titanic

#   Plot a box plot for distribution of age with respect to each gender along with the information about whether they survived or not.
#   This means that - Box Plot x=sex and y=age with hue=survived
plt.figure(figsize=(15,10))
sns.boxplot(data=titanic, x="sex", y="age", hue="alive")


-----------------------------------------------------------------------------------------------------------------

#PR 10
#   import libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
#   Load iris dataset
iris = sns.load_dataset("iris")
iris

#   1. List down the features and their types (e.g., numeric, nominal) available in the dataset.
iris.describe()

iris.info()

#   2. Create a histogram for each feature in the dataset to illustrate the feature distributions. 
plt.figure(figsize=(12,10))
sns.histplot(data=iris, x="species", y="sepal_length", bins=15)

plt.figure(figsize=(12,10))
sns.histplot(data=iris, x="species", y="sepal_width", bins=15)

plt.figure(figsize=(12,10))
sns.histplot(data=iris, x="species", y="petal_length", bins=15)

plt.figure(figsize=(12,10))
sns.histplot(data=iris, x="species", y="petal_width", bins=15)

#   3. Compare distributions and identify outliers
#   For outliers we use Box plot
numerical_col = ['sepal_length','sepal_width','petal_length','petal_width']
categorical_col = ['species']
iris.boxplot(numerical_col)
#   in the below plot, we can see that the outliers lie in the upper and lower bound of the sepal width region

"""
Calculate the first and third quartile (Q1 and Q3).
Further, evaluate the interquartile range, IQR = Q3-Q1.
Estimate the lower bound, the lower bound = Q1*1.5
Estimate the upper bound, upper bound = Q3*1.5
Replace the data points that lie outside of the lower and the upper bound with a NULL value.
"""
for x in ['sepal_width']:
    q75,q25 = np.percentile(iris.loc[:,x],[75,25])
    intr_qr = q75-q25
    max = q75+(1.5*intr_qr)
    min = q25-(1.5*intr_qr)
    iris.loc[iris[x] < min,x] = np.nan
    iris.loc[iris[x] > max,x] = np.nan

print("Sum of count of NULL values/outliers in each column of the dataset:")
iris.isnull().sum()

iris['sepal_width'].describe()
#   since the count is 146 we can use median of sepal_width column to replace the null values
iris['sepal_width'].fillna(iris['sepal_width'].median(), inplace=True)

#   check for null values and outliers again
iris.isnull().sum()

iris.boxplot(numerical_col)
--------------------------------------------------------------------------------------------------------------